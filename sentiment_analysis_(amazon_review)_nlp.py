# -*- coding: utf-8 -*-
"""Sentiment Analysis (Amazon Review) - NLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qVsTRjJ9gqZe6y804_62oF6Sv2P0tNRz
"""

!pip install nltk
!pip install spacy
!python -m spacy download en_core_web_sm
from warnings import filterwarnings
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import nltk
from nltk.corpus import stopwords
from nltk.corpus import stopwords
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate
from sklearn.preprocessing import LabelEncoder
from textblob import Word, TextBlob
from wordcloud import WordCloud
import re
import spacy
from tabulate import tabulate
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix






filterwarnings('ignore')
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 200)
pd.set_option('display.float_format', lambda x: '%.2f' % x)

"""This code snippet first ensures that the NLTK (Natural Language Toolkit) and spaCy libraries are installed, which are commonly used for natural language processing tasks. Then, it calculates the term frequency (tf) of each word in the "reviewText" column of the DataFrame. This is achieved by splitting each review text into individual words and tallying the occurrence of each word across all reviews. The resulting DataFrame contains two columns: "words" and "tf", representing unique words and their corresponding term frequencies, respectively. This analysis provides valuable insights into the distribution and prevalence of words within the dataset, aiding in further exploration and understanding of the textual data."""

# Assuming your file is named "amazon_reviews.csv" and it's located in the "/content/" directory
file_path = "/content/amazon_reviews.csv"

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# Display the first few rows of the DataFrame
df.head()

"""This code reads a CSV file containing Amazon reviews data into a DataFrame using the Pandas library. It assumes the file is named "amazon_reviews.csv" and is located in the "/content/" directory. After reading the file, it displays the first few rows of the DataFrame to provide a preview of the data."""

# Assuming your file is named "amazon_reviews.csv" and it's located in the "/content/" directory
file_path = "/content/amazon_reviews.csv"

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# Display the first few rows of the DataFrame
df.head()

"""To count the number of instance"""

# computing number of rows
rows = len(df.axes[0])

# computing number of columns
cols = len(df.axes[1])

print("Number of Rows: ", rows)
print("Number of Columns: ", cols)

"""df.describe()*italicized text*"""

df.describe()

"""This code snippet converts all text in the 'reviewText' column of the DataFrame to lowercase using the `.str.lower()` method in Pandas. By doing this, it ensures that the text is standardized to lowercase, which can be helpful for subsequent analysis such as text processing or sentiment analysis. The `.head()` function then displays the first few rows of the DataFrame with the updated 'reviewText' column."""

df['reviewTime'] = pd.to_datetime(df['reviewTime'])

# Extract year and month from 'reviewTime' to group by
df['reviewYearMonth'] = df['reviewTime'].dt.to_period('M')

# Count the number of reviews per year-month
reviews_per_month = df.groupby('reviewYearMonth').size()

# Plotting the time series
plt.figure(figsize=(10, 6))
reviews_per_month.plot()
plt.title('Time Series of Number of Reviews')
plt.xlabel('Review Year-Month')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

df['reviewText'] = df['reviewText'].str.lower()
df.head()

"""This code removes any non-alphanumeric characters and whitespace from the 'reviewText' column in the DataFrame. Then, it displays the first few rows of the DataFrame with the cleaned 'reviewText'."""

df['reviewText'] = df['reviewText'].str.replace('[^\w\s]', '')
df.head()

"""This code removes any punctuation characters from the 'reviewText' column in the DataFrame. Then, it displays the first 20 entries of the cleaned 'reviewText' column."""

#Punctuation
df["reviewText"] = [re.sub("[^a-zA-Z]", " ", i) for i in df["reviewText"].astype(str)]
df["reviewText"].head(20)

"""This code removes any digits from the 'reviewText' column in the DataFrame. Then, it displays the first 20 entries of the cleaned 'reviewText' column."""

df['reviewText'] = df['reviewText'].str.replace('\d', '')
df.head(20)

"""This code downloads the NLTK stopwords corpus and prints out the list of English stopwords. Then, it removes the stopwords from the 'reviewText' column in the DataFrame 'df' and displays the last 20 entries of the cleaned 'reviewText' column."""

nltk.download('stopwords')

# Get the list of stopwords

sw = stopwords.words('english')
print(sw)


# Remove stopwords from the 'reviewText' column
df["reviewText"] = df["reviewText"].apply(lambda x: " ".join(x for x in str(x).split() if x not in sw))
print(df["reviewText"].tail(20))

"""This code snippet utilizes the `re` module to remove digits from the 'reviewText' column in the DataFrame 'df'. It applies a regular expression `\d` to replace any digit characters with an empty string. Finally, it prints the last 20 entries of the modified 'reviewText' column."""

# Remove digits from the 'reviewText' column
df["reviewText"] = df["reviewText"].apply(lambda x: re.sub("\d", "", str(x)))

# Print the last few rows of the modified 'reviewText' column
print(df["reviewText"].tail(20))

"""This code removes infrequent words (those that appear only once) from the 'reviewText' column of the DataFrame 'df'. First, it creates a temporary DataFrame `temp_df` containing the word frequency counts using `value_counts()`. Then, it identifies words with a count of 1 or less and stores them in the `drops` variable. Next, it applies a lambda function to filter out these infrequent words from each entry in the 'reviewText' column, using a list comprehension and the `join()` and `split()` methods. Finally, it prints the last 20 entries of the modified 'reviewText' column."""

temp_df = pd.Series(" ".join(df["reviewText"]).split()).value_counts()
print(temp_df)

drops = temp_df[temp_df <=1]
print(drops)
df["reviewText"] = df["reviewText"].apply(lambda x: " ".join(x for x in x.split() if x not in drops))

"""This code snippet first downloads the NLTK tokenizer data required for word tokenization using `nltk.download("punkt")`. Then, it tokenizes the text in the 'reviewText' column using the `TextBlob` library, which is part of the NLTK package. It converts each text entry into a list of words. The resulting DataFrame contains these tokenized words in the 'reviewText' column. However, it seems there's a missing operation in the code, as it's not clear what you want to do after tokenization. If you have any specific tasks or analysis you'd like to perform with this tokenized data, please specify."""

nltk.download("punkt")

df["reviewText"].apply(lambda x: TextBlob(x).words).head(20)
df['reviewText']

"""This code segment utilizes the SpaCy library to perform lemmatization on the text in the "reviewText" column of the DataFrame. First, it loads the English language model using `spacy.load("en_core_web_sm")`. Then, it defines a function called `lemmatize_text()` that takes a text input, processes it using SpaCy's NLP pipeline, and extracts lemmas (base forms) of tokens. Finally, it applies this lemmatization function to each entry in the "reviewText" column of the DataFrame using the `apply()` function. The resulting DataFrame contains the lemmatized text in the "reviewText" column, with the entire text displayed for each entry due to setting the display options."""

# Load the English language model
nlp = spacy.load("en_core_web_sm")

# Define a function to lemmatize text
def lemmatize_text(text):
    doc = nlp(text)
    lemmas = [token.lemma_ for token in doc]
    return " ".join(lemmas)

# Apply the lemmatize_text function to the "reviewText" column of your DataFrame
df["reviewText"] = df["reviewText"].apply(lemmatize_text)
# Set display options to show the entire text
pd.set_option('display.max_colwidth', None)

# Print the entire "reviewText" column along with the first 20 rows
print(df["reviewText"])

"""This code defines a dictionary called `contraction_mapping` that contains common contractions as keys and their expanded forms as values. Then, it defines a function called `expand_contractions()` that takes a text input and expands any contractions found in the text using the provided mapping. The function uses a regular expression pattern to find contractions in the text and replaces them with their expanded forms. Finally, the function is applied to the "reviewText" column of the DataFrame using the `apply()` function, and the expanded text is printed for the first 20 rows."""

contraction_mapping = {
    "ain't": "is not",
    "aren't": "are not",
    "can't": "cannot",
    "can't've": "cannot have",
    "'cause": "because",
    "could've": "could have",
    "couldn't": "could not",
    "couldn't've": "could not have",
    "didn't": "did not",
    "doesn't": "does not",
    "don't": "do not",
    "hadn't": "had not",
    "hadn't've": "had not have",
    "hasn't": "has not",
    "haven't": "have not",
    "he'd": "he would",
    "he'd've": "he would have",
    "he'll": "he will",
    "he'll've": "he will have",
    "he's": "he is",
    "how'd": "how did",
    "how'd'y": "how do you",
    "how'll": "how will",
    "how's": "how is",
    "I'd": "I would",
    "I'd've": "I would have",
    "I'll": "I will",
    "I'll've": "I will have",
    "I'm": "I am",
    "I've": "I have",
    "i'd": "i would",
    "i'd've": "i would have",
    "i'll": "i will",
    "i'll've": "i will have",
    "i'm": "i am",
    "i've": "i have",
    "isn't": "is not",
    "it'd": "it would",
    "it'd've": "it would have",
    "it'll": "it will",
    "it'll've": "it will have",
    "it's": "it is",
    "let's": "let us",
    "ma'am": "madam",
    "mayn't": "may not",
    "might've": "might have",
    "mightn't": "might not",
    "mightn't've": "might not have",
    "must've": "must have",
    "mustn't": "must not",
    "mustn't've": "must not have",
    "needn't": "need not",
    "needn't've": "need not have",
    "o'clock": "of the clock",
    "oughtn't": "ought not",
    "oughtn't've": "ought not have",
    "shan't": "shall not",
    "sha'n't": "shall not",
    "shan't've": "shall not have",
    "she'd": "she would",
    "she'd've": "she would have",
    "she'll": "she will",
    "she'll've": "she will have",
    "she's": "she is",
    "should've": "should have",
    "shouldn't": "should not",
    "shouldn't've": "should not have",
    "so've": "so have",
    "so's": "so is",
    "that'd": "that would",
    "that'd've": "that would have",
    "that's": "that is",
    "there'd": "there would",
    "there'd've": "there would have",
    "there's": "there is",
    "they'd": "they would",
    "they'd've": "they would have",
    "they'll": "they will",
    "they'll've": "they will have",
    "they're": "they are",
    "they've": "they have",
    "to've": "to have",
    "wasn't": "was not",
    "we'd": "we would",
    "we'd've": "we would have",
    "we'll": "we will",
    "we'll've": "we will have",
    "we're": "we are",
    "we've": "we have",
    "weren't": "were not",
    "what'll": "what will",
    "what'll've": "what will have",
    "what're": "what are",
    "what's": "what is",
    "what've": "what have",
    "when's": "when is",
    "when've": "when have",
    "where'd": "where did",
    "where's": "where is",
    "where've": "where have",
    "who'll": "who will",
    "who'll've": "who will have",
    "who's": "who is",
    "who've": "who have",
    "why's": "why is",
    "why've": "why have",
    "will've": "will have",
    "won't": "will not",
    "won't've": "will not have",
    "would've": "would have",
    "wouldn't": "would not",
    "wouldn't've": "would not have",
    "y'all": "you all",
    "y'all'd": "you all would",
    "y'all'd've": "you all would have",
    "y'all're": "you all are",
    "y'all've": "you all have",
    "you'd": "you would",
    "you'd've": "you would have",
}

# Function to expand contractions
def expand_contractions(text, contraction_mapping):
    # Regular expression pattern to find contractions
    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),
                                      flags=re.IGNORECASE|re.DOTALL)

    # Function to replace contractions with their expansions
    def expand_match(contraction):
        match = contraction.group(0)
        first_char = match[0]
        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())
        expanded_contraction = first_char + expanded_contraction[1:]
        return expanded_contraction

    # Replace contractions in text using the pattern and the function to replace
    expanded_text = contractions_pattern.sub(expand_match, text)
    return expanded_text

# Apply the expand_contractions function to the "reviewText" column of your DataFrame
df["reviewText"] = df["reviewText"].apply(lambda x: expand_contractions(x, contraction_mapping))

# Print the entire "reviewText" column along with the first 20 rows
pd.set_option('display.max_colwidth', None)
print(df["reviewText"].head(20))

"""This code defines a function called `normalize_text()` that takes a text input and normalizes URLs and email addresses by replacing them with tokens ('URL' and 'EMAIL' respectively). The function uses regular expressions to identify URLs and email addresses in the text and replaces them with the specified tokens. Then, the function is applied to the "reviewText" column of the DataFrame using the `apply()` function. Finally, the normalized text is printed for the entire "reviewText" column, along with the first 20 rows."""

# Function to normalize URLs, emails, etc.
def normalize_text(text):
    # Replace URLs with a token
    text = re.sub(r'http\S+', 'URL', text)
    # Replace email addresses with a token
    text = re.sub(r'\b[\w\.-]+@[\w\.-]+\.\w+\b', 'EMAIL', text)
    # Add more normalization rules as needed

    return text

# Apply the normalize_text function to the "reviewText" column of your DataFrame
df["reviewText"] = df["reviewText"].apply(normalize_text)

# Print the entire "reviewText" column along with the first 20 rows
pd.set_option('display.max_colwidth', None)
print(df["reviewText"])

"""This code first prints the current columns in the DataFrame. Then, it specifies a list of columns to remove, including 'unixReviewTime', 'reviewerName', 'asin', 'day_diff', and 'reviewerID'. After that, it drops these specified columns from the DataFrame using the `drop()` function with the `columns` parameter. Finally, it prints the remaining columns in the DataFrame and displays the top 20 rows to verify the changes."""

# Print the current columns in the DataFrame
print("Current Columns:")
print(df.columns)

# List of columns to remove
columns_to_remove = ['unixReviewTime', 'reviewerName', 'asin', 'day_diff','reviewerID']

# Drop the specified columns from the DataFrame
df.drop(columns=columns_to_remove, inplace=True)

# Print the remaining columns
remaining_columns = df.columns
print("\nRemaining Columns:")
print(remaining_columns)

# Display the top 20 rows of the DataFrame
print("\nTop 20 Rows:")
print(df.head(20))

"""This code utilizes the `tabulate` library to print the remaining columns of the DataFrame in a table format. It first creates a DataFrame containing the remaining columns, then prints the table using the `tabulate` function with the specified headers and table format. This format makes it easier to visualize the remaining columns."""

# Print the remaining columns in a table format
print("Remaining Columns:")
print(tabulate(pd.DataFrame(df.columns, columns=['Columns']), headers='keys', tablefmt='psql'))

# Print 10 entries from the DataFrame
print("Top 10 Entries:")
print(tabulate(df.head(10), headers='keys', tablefmt='psql'))

"""This code recalculates the helpfulness ratio by dividing the 'helpful_yes' column by the 'total_vote' column and then prints the correlation between 'helpful_yes' and 'total_vote'. After that, it visualizes the distribution of the helpfulness ratio using a histogram with a kernel density estimation (KDE) plot overlaid. The histogram shows the frequency distribution of the helpfulness ratio values, while the KDE plot provides a smooth estimate of the probability density function. This visualization helps to understand how helpful the reviews are relative to the total number of votes they received."""

# Recalculate the helpfulness ratio
df['helpfulness_ratio'] = df['helpful_yes'] / df['total_vote']

# Print the correlation between 'helpful_yes' and 'total_vote'
correlation = df['helpful_yes'].corr(df['total_vote'])
print("Correlation between helpful_yes and total_vote:", correlation)

# Visualize the distribution of helpfulness ratio
plt.figure(figsize=(8, 6))
sns.histplot(df['helpfulness_ratio'], bins=30, kde=True, color='skyblue')
plt.title('Distribution of Helpfulness Ratio')
plt.xlabel('Helpfulness Ratio')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""This code calculates the correlation coefficient between the 'helpful_yes' and 'total_vote' columns in the DataFrame 'df', which indicates the strength and direction of their linear relationship. The correlation coefficient ranges from -1 to 1, where:

- 1 indicates a perfect positive correlation,
- -1 indicates a perfect negative correlation, and
- 0 indicates no correlation.

The printed output shows the correlation coefficient value between the two columns.
"""

# Assuming 'helpful_yes' and 'total_vote' columns are present in your DataFrame 'df'

# Calculate the correlation between 'helpful_yes' and 'total_vote'
correlation = df['helpful_yes'].corr(df['total_vote'])

# Print the correlation coefficient
print("Correlation between 'helpful_yes' and 'total_vote':", correlation)

"""This code performs a time series analysis of the mean overall ratings over months for each year present in the DataFrame 'df'. It first converts the 'reviewTime' column to datetime format and extracts the year and month from it. Then, it groups the data by year and month, calculating the mean overall rating for each month. Finally, it plots the time series of mean overall ratings over months within each year, showing how the ratings change over time. Each year's data is represented by a separate line plot with markers indicating individual data points. The plot includes labels, a title, and a legend to aid interpretation."""

# Assuming 'overall' and 'reviewTime' columns are present in your DataFrame 'df'

# Convert 'reviewTime' column to datetime format
df['reviewTime'] = pd.to_datetime(df['reviewTime'])

# Extract year and month from 'reviewTime' column
df['year'] = df['reviewTime'].dt.year
df['month'] = df['reviewTime'].dt.month

# Group the data by year and month and calculate the mean overall rating for each month
overall_ratings_monthly = df.groupby(['year', 'month'])['overall'].mean()

# Plot the time series of mean overall ratings over months within each year
plt.figure(figsize=(12, 6))
for year in overall_ratings_monthly.index.levels[0]:
    data = overall_ratings_monthly.loc[year]
    plt.plot(data.index, data.values, marker='o', label=year)

plt.title('Mean Overall Ratings Over Months')
plt.xlabel('Month')
plt.ylabel('Mean Overall Rating')
plt.legend(title='Year')
plt.grid(True)
plt.xticks(range(1, 13))
plt.show()

"""This code defines a function `extract_first_value` that extracts the first value from a list represented as a string. It then applies this function to each value in the 'helpful' column of the DataFrame 'df', converting the string representation into an integer and summing up the results. Finally, it prints the total number of helpful votes. If there's any error encountered during conversion, it handles it gracefully by returning 0."""

# Function to extract the first value from a list in a string representation
def extract_first_value(string_list):
    try:
        return int(string_list.strip('[]').split(',')[0])
    except ValueError:
        return 0

# Apply the function to each value in the 'helpful' column and sum up the results
total_helpful_votes = df['helpful'].apply(extract_first_value).sum()

print("Total Helpful Votes:", total_helpful_votes)

"""This code defines a function `extract_second_value` that extracts the second value from a list represented as a string. It then applies this function to each value in the 'helpful' column of the DataFrame 'df', converting the string representation into an integer and summing up the results. Finally, it prints the total number of unhelpful votes. If there's any error encountered during conversion, it handles it gracefully by returning 0."""

# Function to extract the second value from a list in a string representation
def extract_second_value(string_list):
    try:
        return int(string_list.strip('[]').split(',')[1])
    except ValueError:
        return 0

# Apply the function to each value in the 'helpful' column and sum up the results
total_unhelpful_votes = df['helpful'].apply(extract_second_value).sum()

print("Total Unhelpful Votes:", total_unhelpful_votes)

"""This code creates a pie chart to visualize the distribution of helpful and unhelpful votes. It sets up the data with the total counts of helpful and unhelpful votes, defines the labels for the pie chart, and specifies colors for each section. Then, it creates the pie chart using Matplotlib, with percentages displayed on each section and a title indicating the distribution of votes. Finally, it displays the pie chart. If any of the sizes are negative, it prints an error message indicating that sizes must be non-negative."""

# Total helpful and unhelpful votes
total_helpful_votes = 6444
total_unhelpful_votes = 7478

# Data for the pie chart
labels = ['Helpful', 'Unhelpful']
sizes = [total_helpful_votes, total_unhelpful_votes]

# Check if sizes are non-negative
if any(size < 0 for size in sizes):
    print("Error: Sizes must be non-negative.")
else:
    # Set up the colors for the pie chart
    colors = ['#ff9999', '#66b3ff']

    # Create the pie chart
    plt.figure(figsize=(8, 6))
    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
    plt.title('Distribution of Helpful and Unhelpful Votes')
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.show()

"""This code calculates the number of unique words in the DataFrame's "words" column using the `nunique()` function. It returns the count of unique words present in the column."""

df["reviewText"].nunique()

"""This code computes the descriptive statistics for the "tf" column of the DataFrame `tf`. It includes various percentiles such as 5th, 10th, 25th, 50th (median), 75th, 80th, 90th, 95th, and 99th percentiles. The `.T` at the end transposes the result, making it easier to read."""

df.columns

"""This code generates a bar plot using the DataFrame `tf` filtered for words with a term frequency greater than 500. It plots the selected words on the x-axis and their corresponding term frequencies on the y-axis. Finally, it displays the bar plot using `plt.show()`. This visualization helps identify the most frequent words in the dataset.

This code creates a word cloud visualization using the review text data from the DataFrame `df`. It concatenates all the review texts into a single string variable named `text`. Then, it generates a word cloud using the `WordCloud` class and the `generate()` method, passing the `text` variable as input. Finally, it displays the word cloud using `plt.imshow()` and sets the axis off to remove axis labels, followed by `plt.show()` to show the word cloud plot. Word clouds are useful for visualizing the most frequent words in a text corpus.
"""

text = " ".join(i for i in df.reviewText)
wordcloud = WordCloud().generate(text)
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

"""This code generates a word cloud with specific settings. It sets the maximum font size to 50 using the `max_font_size` parameter, limits the maximum number of words in the cloud to 100 with the `max_words` parameter, and sets the background color to white using `background_color="white"`. Then, it creates the word cloud using the `WordCloud` class with the specified settings and generates it from the `text` variable. Finally, it displays the word cloud plot with `plt.imshow()` and hides the axis labels with `plt.axis("off")`, followed by `plt.show()` to show the plot. This customization allows for more control over the appearance of the word cloud."""

wordcloud = WordCloud(max_font_size=50,
                      max_words=100,
                      background_color="white").generate(text)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()



"""Using confusion matrix to show incorrect and correct pridiction."""

print(df.columns)

df.info()

df.isnull().sum()

"""This Python code utilizes the NLTK library to conduct sentiment analysis on a DataFrame containing review texts. It begins by importing the SentimentIntensityAnalyzer class from NLTK. Next, it initializes an instance of this class and defines a function called `get_sentiment` to determine the sentiment (positive, negative, or neutral) of a given text input. Within this function, the SentimentIntensityAnalyzer's polarity_scores method calculates polarity scores, which are then used to classify the text's sentiment. These sentiments are then assigned to each review text in the DataFrame, with the distribution of sentiment values subsequently printed out. Overall, the code streamlines sentiment analysis on a collection of reviews, providing insights into the prevailing sentiments within the dataset."""

# Download the vader_lexicon
nltk.download('vader_lexicon')


# Initialize SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

# Sample DataFrame (replace this with your actual DataFrame)
# Assuming your DataFrame is named 'df' and contains the 'reviewText' column
# df = ...

# Function to get sentiment polarity
def get_sentiment(text):
    sentiment = sia.polarity_scores(text)
    if sentiment['compound'] > 0.05:
        return 'Positive'
    elif sentiment['compound'] < -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Apply sentiment analysis to each review and add a new column 'sentiment'
df['sentiment'] = df['reviewText'].apply(get_sentiment)

# Count occurrences of each sentiment category
sentiment_counts = df['sentiment'].value_counts()

# Print the top 20 rows including the new 'sentiment' column
print(df[['reviewText', 'sentiment']].head(20))

# Print sentiment counts
print("\nSentiment Counts:")
print(sentiment_counts)

# Vectorize the text data
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['reviewText'])

# Apply RandomUnderSampler for under-sampling the majority class
undersampler = RandomUnderSampler(random_state=42)
X_undersampled, y_undersampled = undersampler.fit_resample(X, df['sentiment'])

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_undersampled, y_undersampled)

# Convert the resampled features to an array
X_resampled_array = X_resampled.toarray()

# Create a DataFrame with balanced data
balanced_df = pd.DataFrame(X_resampled_array, columns=vectorizer.get_feature_names_out())
balanced_df['sentiment'] = y_resampled

# Print the shape of the balanced dataset
print("Balanced Dataset Shape:", balanced_df.shape)

# Split the balanced data into features (X_balanced) and target (y_balanced)
X_balanced = balanced_df.drop('sentiment', axis=1)
y_balanced = balanced_df['sentiment']

# Split the balanced data into train and test sets
X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

# Initialize classifiers
classifiers = {
    "Logistic Regression": LogisticRegression(),
    "Linear SVM": SVC(kernel='linear'),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier()
}

# Test each classifier
for classifier_name, classifier in classifiers.items():
    # Train the classifier
    classifier.fit(X_train_balanced, y_train_balanced)

    # Predictions
    y_pred = classifier.predict(X_test_balanced)

    # Calculate accuracy
    accuracy = accuracy_score(y_test_balanced, y_pred)

    # Print accuracy
    print(f"{classifier_name} Model Accuracy: {accuracy}")

    # Print classification report
    print(f"Classification Report for {classifier_name} Model:")
    report = classification_report(y_test_balanced, y_pred)
    print(report)

"""The Logistic Regression model achieved a moderate level of accuracy (70.05%) and demonstrated balanced precision, recall, and F1-scores across the three classes (Negative, Neutral, Positive). It provided a reliable baseline performance but showed some room for improvement, particularly in recall for the Neutral class.

The Linear SVM model outperformed Logistic Regression with an accuracy of 71.50%. It showed improved precision and recall for the Positive class, making it more suitable for tasks where correctly identifying positive sentiments is crucial. However, it exhibited slightly lower precision for the Neutral class compared to Logistic Regression.

The Decision Tree model yielded the lowest accuracy (64.73%) among the tested models. While it demonstrated balanced precision and recall for the Positive class, it struggled with predicting the Negative class, indicating potential limitations in capturing the underlying patterns in the data.

The Random Forest model achieved a competitive accuracy of 70.53% and demonstrated balanced performance across the classes. It showed notable improvement in recall for the Neutral class compared to Logistic Regression, suggesting its potential for handling imbalanced datasets. However, like Logistic Regression, it struggled with recall for the Negative class.

Overall, the Linear SVM model emerged as the best-performing model, offering a good balance of accuracy and precision-recall trade-offs across all classes. However, further optimization and fine-tuning of the models may be necessary to achieve even better performance and balance across all classes.
"""

# Test each classifier
for classifier_name, classifier in classifiers.items():
    # Train the classifier
    classifier.fit(X_train_balanced, y_train_balanced)

    # Predictions
    y_pred = classifier.predict(X_test_balanced)

    # Calculate accuracy
    accuracy = accuracy_score(y_test_balanced, y_pred)

    # Print accuracy
    print(f"{classifier_name} Model Accuracy: {accuracy}")

    # Print classification report
    print(f"Classification Report for {classifier_name} Model:")
    report = classification_report(y_test_balanced, y_pred)
    print(report)

    # Calculate confusion matrix
    cm = confusion_matrix(y_test_balanced, y_pred)

    # Plot confusion matrix
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g',
                xticklabels=classifier.classes_, yticklabels=classifier.classes_)
    plt.title(f'Confusion Matrix for {classifier_name} Model')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.show()

# Split the balanced data into features (X_balanced) and target (y_balanced)
X_balanced = balanced_df.drop('sentiment', axis=1)
y_balanced = balanced_df['sentiment']

# Split the balanced data into train and test sets
X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)

# Initialize classifiers
classifiers = {
    "Logistic Regression": LogisticRegression(),
    "Linear SVM": SVC(kernel='linear'),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier()
}

# Test each classifier
for classifier_name, classifier in classifiers.items():
    # Perform cross-validation
    cross_val_scores = cross_val_score(classifier, X_balanced, y_balanced, cv=5, scoring='accuracy')

    # Calculate mean cross-validation score
    mean_cv_score = np.mean(cross_val_scores)

    # Print mean cross-validation score
    print(f"Mean Cross-Validation Score for {classifier_name}:", mean_cv_score)

    # Train the classifier
    classifier.fit(X_train_balanced, y_train_balanced)

    # Predictions
    y_pred = classifier.predict(X_test_balanced)

    # Calculate accuracy
    accuracy = accuracy_score(y_test_balanced, y_pred)

    # Print accuracy
    print(f"{classifier_name} Model Accuracy: {accuracy}")

    # Print classification report
    print(f"Classification Report for {classifier_name} Model:")
    report = classification_report(y_test_balanced, y_pred)
    print(report)

"""For the Decision Tree model, the initial accuracy was 0.65, but after cross-validation, it improved to 0.68. Additionally, the f1-score for each class also shows improvement, particularly for the Negative class, where precision, recall, and f1-score all increased. This indicates better performance in correctly classifying instances of this class.

Similarly, the Random Forest model showed improvement in accuracy from 0.72 to 0.73 after cross-validation. The precision, recall, and f1-score for each class also saw enhancements, particularly in the Negative class, where precision and f1-score increased noticeably. This suggests better handling of instances in this class.

Overall, the output suggests that the SVM model trained with sentiment features performs exceptionally well in classifying reviews into sentiment categories. It achieves high accuracy and balanced performance across different sentiment classes, demonstrating the effectiveness of incorporating sentiment features for sentiment analysis task
"""

# Initialize SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()

# Sample DataFrame (replace this with your actual DataFrame)
# Assuming your DataFrame is named 'df' and contains the 'reviewText', 'reviewTime' columns
# df = ...

# Function to get sentiment polarity
def get_sentiment(text):
    sentiment = sia.polarity_scores(text)
    if sentiment['compound'] > 0.05:
        return 'Positive'
    elif sentiment['compound'] < -0.05:
        return 'Negative'
    else:
        return 'Neutral'

# Apply sentiment analysis to each review and add a new column 'sentiment'
df['sentiment'] = df['reviewText'].apply(get_sentiment)

# Convert 'reviewTime' to datetime format
df['reviewTime'] = pd.to_datetime(df['reviewTime'])

# Extract year and month from 'reviewTime'
df['year'] = df['reviewTime'].dt.year
df['month'] = df['reviewTime'].dt.month

# Group by year and month, then count occurrences of each sentiment category
sentiment_counts_over_time = df.groupby(['year', 'month', 'sentiment']).size().unstack(fill_value=0)

# Plot sentiment trends over time
sentiment_counts_over_time.plot(kind='line', figsize=(10, 6), marker='o')
plt.title('Sentiment Trends Over Time')
plt.xlabel('Year-Month')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=45)
plt.grid(True)
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()

